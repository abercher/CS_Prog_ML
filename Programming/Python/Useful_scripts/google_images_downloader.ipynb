{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has been take from: https://github.com/hardikvasa/google-images-download/blob/master/google-images-download.py\n",
    "but since then it has been modified on github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "# Searching and Downloading Google Images/Image Links\n",
    "\n",
    "# Import Libraries\n",
    "# coding: UTF-8\n",
    "import time  # Importing the time library to check the time of code execution\n",
    "import sys  # Importing the System Library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Edit From Here ###########\n",
    "\n",
    "# This list is used to search keywords. You can edit this list to search for google images of your choice. You can simply add and remove elements of the list.\n",
    "search_keyword = ['male human']\n",
    "\n",
    "# This list is used to further add suffix to your search term. Each element of the list will help you download 100 images. First element is blank which denotes that no suffix is added to the search keyword of the above list. You can edit the list by adding/deleting elements from it.So if the first element of the search_keyword is 'Australia' and the second element of keywords is 'high resolution', then it will search for 'Australia High Resolution'\n",
    "keywords = [' high resolution']\n",
    "\n",
    "\n",
    "########### End of Editing ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading entire Web Document (Raw Page Content)\n",
    "def download_page(url):\n",
    "    version = (3, 0)\n",
    "    cur_version = sys.version_info\n",
    "    if cur_version >= version:  # If the Current Version of Python is 3.0 or above\n",
    "        import urllib.request  # urllib library for Extracting web pages\n",
    "        try:\n",
    "            headers = {}\n",
    "            headers[\n",
    "                'User-Agent'] = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "            req = urllib.request.Request(url, headers=headers)\n",
    "            resp = urllib.request.urlopen(req)\n",
    "            respData = str(resp.read())\n",
    "            return respData\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "    else:  # If the Current Version of Python is 2.x\n",
    "        import urllib2\n",
    "        try:\n",
    "            headers = {}\n",
    "            headers[\n",
    "                'User-Agent'] = \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"\n",
    "            req = urllib2.Request(url, headers=headers)\n",
    "            response = urllib2.urlopen(req)\n",
    "            page = response.read()\n",
    "            return page\n",
    "        except:\n",
    "            return \"Page Not found\"\n",
    "\n",
    "\n",
    "# Finding 'Next Image' from the given raw page\n",
    "def _images_get_next_item(s):\n",
    "    start_line = s.find('rg_di')\n",
    "    if start_line == -1:  # If no links are found then give an error!\n",
    "        end_quote = 0\n",
    "        link = \"no_links\"\n",
    "        return link, end_quote\n",
    "    else:\n",
    "        start_line = s.find('\"class=\"rg_meta\"')\n",
    "        start_content = s.find('\"ou\"', start_line + 1)\n",
    "        end_content = s.find(',\"ow\"', start_content + 1)\n",
    "        content_raw = str(s[start_content + 6:end_content - 1])\n",
    "        return content_raw, end_content\n",
    "\n",
    "\n",
    "# Getting all links with the help of '_images_get_next_image'\n",
    "def _images_get_all_items(page):\n",
    "    items = []\n",
    "    while True:\n",
    "        item, end_content = _images_get_next_item(page)\n",
    "        if item == \"no_links\":\n",
    "            break\n",
    "        else:\n",
    "            items.append(item)  # Append all the links in the list named 'Links'\n",
    "            time.sleep(0.1)  # Timer could be used to slow down the request for image downloads\n",
    "            page = page[end_content:]\n",
    "    return items\n",
    "\n",
    "\n",
    "############## Main Program ############\n",
    "t0 = time.time()  # start the timer\n",
    "\n",
    "\n",
    "version = (3,0)\n",
    "cur_version = sys.version_info\n",
    "if cur_version >= version:  # If the Current Version of Python is 3.0 or above\n",
    "    # urllib library for Extracting web pages\n",
    "    #import urllib.request\n",
    "    from urllib.request import Request, urlopen\n",
    "    from urllib.request import URLError, HTTPError\n",
    "\n",
    "else:  # If the Current Version of Python is 2.x\n",
    "    # urllib library for Extracting web pages\n",
    "    from urllib2 import Request, urlopen\n",
    "    from urllib2 import URLError, HTTPError\n",
    "\n",
    "# Download Image Links\n",
    "i = 0\n",
    "while i < len(search_keyword):\n",
    "    items = []\n",
    "    iteration = \"Item no.: \" + str(i + 1) + \" -->\" + \" Item name = \" + str(search_keyword[i])\n",
    "    print (iteration)\n",
    "    print (\"Evaluating...\")\n",
    "    search_keywords = search_keyword[i]\n",
    "    search = search_keywords.replace(' ', '%20')\n",
    "\n",
    "    # make a search keyword  directory\n",
    "    try:\n",
    "        os.makedirs(search_keywords)\n",
    "    except OSError as e:\n",
    "        if e.errno != 17:\n",
    "            raise\n",
    "            # time.sleep might help here\n",
    "        pass\n",
    "\n",
    "    j = 0\n",
    "    while j < len(keywords):\n",
    "        pure_keyword = keywords[j].replace(' ', '%20')\n",
    "        url = 'https://www.google.com/search?q=' + search + pure_keyword + '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg'\n",
    "        raw_html = (download_page(url))\n",
    "        #raw_html = (urllib.request.urlopen(url))\n",
    "        time.sleep(0.1)\n",
    "        items = items + (_images_get_all_items(raw_html))\n",
    "        j = j + 1\n",
    "    # print (\"Image Links = \"+str(items))\n",
    "    print (\"Total Image Links = \" + str(len(items)))\n",
    "    print (\"\\n\")\n",
    "\n",
    "    # This allows you to write all the links into a test file. This text file will be created in the same directory as your code. You can comment out the below 3 lines to stop writing the output to the text file.\n",
    "    info = open('output.txt', 'a')  # Open the text file called database.txt\n",
    "    info.write(str(i) + ': ' + str(search_keyword[i - 1]) + \": \" + str(items) + \"\\n\\n\\n\")  # Write the title of the page\n",
    "    info.close()  # Close the file\n",
    "\n",
    "    t1 = time.time()  # stop the timer\n",
    "    total_time = t1 - t0  # Calculating the total time required to crawl, find and download all the links of 60,000 images\n",
    "    print(\"Total time taken: \" + str(total_time) + \" Seconds\")\n",
    "    print (\"Starting Download...\")\n",
    "\n",
    "    ## To save imges to the same directory\n",
    "    # IN this saving process we are just skipping the URL if there is any error\n",
    "\n",
    "    k = 0\n",
    "    errorCount = 0\n",
    "    while (k < len(items)):\n",
    "        try:\n",
    "            req = Request(items[k], headers={\n",
    "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17\"})\n",
    "            response = urlopen(req, None, 15)\n",
    "            image_name = items[k][(items[k].rfind('/'))+1:]\n",
    "            if '?' in image_name:\n",
    "                image_name = image_name[:image_name.find('?')]\n",
    "\n",
    "            output_file = open(search_keywords + \"/\" + str(k + 1) + \". \" + image_name, 'wb')\n",
    "            data = response.read()\n",
    "            output_file.write(data)\n",
    "            response.close()\n",
    "\n",
    "            print(\"completed ====> \" + str(k + 1) + \". \" + image_name)\n",
    "\n",
    "            k = k + 1\n",
    "\n",
    "        except IOError:  # If there is any IOError\n",
    "\n",
    "            errorCount += 1\n",
    "            print(\"IOError on image \" + str(k + 1))\n",
    "            k = k + 1\n",
    "\n",
    "        except HTTPError as e:  # If there is any HTTPError\n",
    "\n",
    "            errorCount += 1\n",
    "            print(\"HTTPError\" + str(k))\n",
    "            k = k + 1\n",
    "        except URLError as e:\n",
    "\n",
    "            errorCount += 1\n",
    "            print(\"URLError \" + str(k))\n",
    "            k = k + 1\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Everything downloaded!\")\n",
    "\n",
    "# ----End of the main program ----#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
